{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tools - pandas**\n",
    "\n",
    "*The `pandas` library provides high-performance, easy-to-use data structures and data analysis tools. The main data structure is the `DataFrame`, which you can think of as an in-memory 2D table (like a spreadsheet, with column names and row labels). Many features available in Excel are available programmatically, such as creating pivot tables, computing columns based on other columns, plotting graphs, etc. You can also group rows by column value, or join tables much like in SQL. Pandas is also great at handling time series.*\n",
    "\n",
    "* Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "built on top of the Python programming language.\n",
    "\n",
    "https://pandas.pydata.org/about/index.html\n",
    "\n",
    "Prerequisites:\n",
    "* NumPy â€“ if you are not familiar with NumPy, I recommend that you go through the NumPy-Guide now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/tools_pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml2/blob/master/tools_pandas.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import `pandas`. People usually import it as `pd`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from xml.etree.ElementInclude import include\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Series` objects\n",
    "The `pandas` library contains these useful data structures:\n",
    "* `Series` objects, that we will discuss now. A `Series` object is 1D array, similar to a column or a row in a spreadsheet (with a column name and row labels).\n",
    "* `DataFrame` objects. This is a 2D table, similar to a spreadsheet (with column names and row labels).\n",
    "* `Panel` objects. You can see a `Panel` as a dictionary of `DataFrame`s. These are less used, so we will not discuss them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `Series`\n",
    "Let's start by creating our first `Series` object!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- **Series from Lists**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# string\n",
    "country = ['India','Pakistan','USA','Nepal','Britain' , 'UAE' , 'Oman']\n",
    "\n",
    "a = pd.Series(country)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    - Hence a series object contains two things index and its value ."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a.dtype",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    - data type is object ('O') that is almost equivalent to strings in python ."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# integers\n",
    "s = pd.Series([2,-1,3,5])\n",
    "s"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "s.dtype",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar to a 1D `ndarray`\n",
    "`Series` objects behave much like one-dimensional NumPy `ndarray`s, and you can often pass them as parameters to NumPy functions:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "np.exp(s)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arithmetic operations on `Series` are also possible, and they apply *elementwise*, just like for `ndarray`s:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "s + [1000,2000,3000,4000]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Similar to NumPy, if you add a single number to a `Series`, that number is added to all items in the `Series`. This is called * broadcasting*:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "s + 1000"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same is true for all binary operations such as `*` or `/`, and even conditional operations:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "s < 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index labels\n",
    "Each item in a `Series` object has a unique identifier called the *index label*. By default, it is simply the rank of the item in the `Series` (starting at `0`) but you can also set the index labels manually:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "s2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\n",
    "s2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the `Series` just like a `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "s2[\"bob\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access the items by integer location, like in a regular array but you must use the `iloc` attribute:\n",
    "- Accessing s2[1] tries to find the label '1', not the position. Use s2.iloc[1] for position-based access."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "s2.iloc[1]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it clear when you are accessing by label or by integer location, it is recommended to always use the `loc` attribute when accessing by label, and the `iloc` attribute when accessing by integer location:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "s2.loc[\"bob\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "s2.iloc[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing a `Series` also slices the index labels:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "s2.iloc[1:3]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can lead to unexpected results when using the default numeric labels, so be careful:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "surprise = pd.Series([1000, 1001, 1002, 1003])\n",
    "surprise"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "surprise_slice = surprise[2:]\n",
    "surprise_slice"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh look! The first element has index label `2`. The element with index label `0` is absent from the slice:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    surprise_slice[0]\n",
    "except KeyError as e:\n",
    "    print(\"Key error:\", e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember that you can access elements by integer location using the `iloc` attribute. This illustrates another reason why it's always better to use `loc` and `iloc` to access `Series` objects:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "surprise_slice.iloc[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- **Naming a series**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "std = pd.Series([56,77,55,78,67,56] , index = ('alice' , 'James' , 'Rahul' ,'Arjun' ,'shantanu','Yukti' ) , name = 'weight of Students' )\n",
    "std"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To add a name to the index of a pandas Series, set the index.name attribute:\n",
    "std.index.name = \"students\"\n",
    "std"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(std.name)             # name of the series\n",
    "print(std.index.name)       # name of the index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Difference Between a series name (weight of Students) and std variable name\n",
    "- std is a variable name which is used to access the series object whereas 'weight of Students' is a name of the series object which can be accessed by std.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a Series with a `dict`\n",
    "You can create a `Series` object from a `dict`. The keys will be used as index labels:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "weights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\n",
    "s3 = pd.Series(weights)\n",
    "s3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control which elements you want to include in the `Series` and in what order by explicitly specifying the desired `index`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "s4 = pd.Series(weights, index = [\"colin\", \"alice\"])\n",
    "s4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Series Attributes"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# size\n",
    "std.size"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# dtype\n",
    "std.dtype"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# name : name of the series .\n",
    "std.name"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# index.name : name of the index .\n",
    "std.index.name"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# is_unique : checks if all the elements in the series are unique or not\n",
    "std.is_unique"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# index : gives the index of the series .\n",
    "std.index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "s.index",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# values : gives the values of the series .\n",
    "std.values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(std.values)    # it returns a numpy array .",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic alignment\n",
    "When an operation involves multiple `Series` objects, `pandas` automatically aligns items by matching index labels."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "s2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "s3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(s2.keys())\n",
    "print(s3.keys())\n",
    "\n",
    "s2 + s3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting `Series` contains the union of index labels from `s2` and `s3`. Since `\"colin\"` is missing from `s2` and `\"charles\"` is missing from `s3`, these items have a `NaN` result value. (ie. Not-a-Number means *missing*).\n",
    "\n",
    "Automatic alignment is very handy when working with data that may come from various sources with varying structure and missing items. But if you forget to set the right index labels, you can have surprising results:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "s5 = pd.Series([1000,1000,1000,1000])\n",
    "print(\"s2 =\", s2.values)\n",
    "print(\"s5 =\", s5.values)\n",
    "\n",
    "s2 + s5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas could not align the `Series`, since their labels do not match at all, hence the full `NaN` result."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initializing a Series with a Scalar Value\n",
    "You can also initialize a `Series` object using a scalar and a list of index labels: all items will be set to the scalar."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "meaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\n",
    "meaning"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Series using read_csv\n",
    "### With one col"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "views = pd.read_csv('../DataSets/VideoViewsGrowth.csv')\n",
    "print(type(views))      # it returns a dataframe object\n",
    "print(views)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- So , pd.read_csv() by default returns a dataframe object .\n",
    "- Now to convert it into a series object we can use `.squeeze()` or use `.iloc[:, 0]` after reading ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "views = pd.read_csv('../DataSets/VideoViewsGrowth.csv').squeeze()\n",
    "print(type(views))  # it returns a series object\n",
    "print(views)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "views = pd.read_csv('../DataSets/VideoViewsGrowth.csv').iloc[:, 0]\n",
    "# can also use .iloc[:, 0] to convert dataframe to series .\n",
    "print(type(views))  # it returns a series object\n",
    "print(views)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### With 2 cols"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VK = pd.read_csv('../DataSets/kohli_ipl.csv')\n",
    "print(type(VK))\n",
    "# it returns a dataframe object .\n",
    "print(VK)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VK = pd.read_csv('../DataSets/kohli_ipl.csv', index_col= 0).squeeze()\n",
    "print(type(VK))\n",
    "# It returns a series object .\n",
    "print(VK)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VK = pd.read_csv('../DataSets/kohli_ipl.csv', index_col=0).iloc[:, 0]\n",
    "print(type(VK))  # it returns a Series object\n",
    "print(VK)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- As shown, the name of the Series is `runs`, which comes from the heading of the second column in the CSV file. This name is used as the label for the values in the Series.\n",
    "- The code loads the CSV, sets the first column as the index, and selects the first data column as a Series because of index_col = 0 ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### With 3 col"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "movies = pd.read_csv('../DataSets/top_500_movies.csv')\n",
    "print(type(movies)) # it returns a dataframe object ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert dataframe to series using squeeze() or iloc[:, col_index]\n",
    "movies_series = pd.read_csv('../DataSets/top_500_movies.csv', index_col=0).squeeze()\n",
    "print(type(movies_series))  # it returns a series object .\n",
    "print(movies_series)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "movies_series_using_iloc = pd.read_csv('../DataSets/top_500_movies.csv', index_col=0).iloc[:, 0]\n",
    "print(type(movies_series))  # it returns a series object .\n",
    "print(movies_series)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- So we have used open_csv to read a csv file and convert it into a series object , that is not its default behaviour ."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Series methods"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`head and tail`\n",
    "- They are used to get a quick overview of the data ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series.head()    # by default it returns first 5 rows , we can also pass a number to it to get that many rows from top .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series.head(8)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series.tail()    # by default it returns last 5 rows , we can also pass a number to it to get that many rows from bottom .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`sample`\n",
    "- it is used to get a random sample of items from an axis of object when there is some bias in the data ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series.sample()  # it returns a random row from the series , here also we can pass a number to it to get that many random rows .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`value_counts`\n",
    "- It helps to get a count of unique values in the series , basically frequency of unique values in the series .\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series.value_counts()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`sort_values`\n",
    "- It helps to sort the series values in ascending or descending order ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series.sort_values(ascending= False)   # by default, it sorts in ascending order , here we have set this to false for sorting in descending order .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VK.sort_values(ascending=False).head(1).values[0]       # This is known as method chaining , where we use multiple methods in a single line of code .\n",
    "# this gives the maximum runs scored by Virat in an IPL match ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series.sort_index(ascending=False , inplace=True)    # this will sort the series based on its index labels in descending order and make it permanent using inplace = True argument .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VK.sort_values(ascending= False)    # this is a temporary sorting , it does not change the original series object .\n",
    "# To make it permanent we can use inplace = True argument ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VK = VK.sort_values(ascending= False)       # this is the correct way to make the sorting permanent .\n",
    "VK"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`sort_index`\n",
    "- It helps to sort the series based on its index labels ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VK.sort_values(inplace=True , ascending= False)\n",
    "# This line attempts to sort the VK Series in place, but the inplace parameter is deprecated for pandas Series. It will raise an error in recent pandas versions. Use assignment instead."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Be careful while using inplace = True argument as it modifies the original series object and this operation is irreversible ."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Series Maths Methods"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`count()`\n",
    "- It returns the count of non-null values in the series , while in numpy we use .size to get the count of all values including null values ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`sum() and prod()`\n",
    "- sum() : returns the sum of all non-null values in the series .\n",
    "- prod() : returns the product of all non-null values in the series ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK.count()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK.sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK.prod()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`mean() , median() , mode() , std() and var()`"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(VK.mean())\n",
    "print(VK.median())\n",
    "print(VK.mode())    # it returns a series object containing all the modes in the series .\n",
    "print(movies_series.mode())   # here we have multiple modes in the series .\n",
    "print(VK.std())\n",
    "print(VK.var())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`min() and max()`\n",
    "- min() : returns the minimum value in the series .\n",
    "- max() : returns the maximum value in the series ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(VK.min())\n",
    "print(VK.max())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`describe()`\n",
    "- It returns a summary of multiple maths methods at once ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK.describe()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Series Indexing\n",
    "- integer indexing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = pd.Series([12,13,14,35,46,57,58,79,9])\n",
    "# positive indexing\n",
    "x[2]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# negative indexing\n",
    "# x[-2]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Negative indexing does not work in pandas series as it does in numpy arrays and python lists , we can use .iloc attribute to access elements using negative indexing ."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x.iloc[-2]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies.iloc[0] # iloc is used for integer based indexing , it stands for integer location .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series.iloc[-3]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series['3 Idiots']",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Series slicing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK[5:11]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK[::50]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK[-7:]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### fancy indexing in series"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK[[1,2,7,5,12,37,39]]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(movies)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# movies['3 Idiots']\n",
    "# not working because it is a dataframe not a series ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# indexing with label , here label means custom index of the series object .\n",
    "movies_series['John Wick: Chapter 2']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Editing Series"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "std",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "std.iloc[0] = 86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "std",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "std['shantanu'] = 71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "std",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# what if an index does not exist\n",
    "std['eve'] = 90"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "std",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Then a new index is created with the given value .\n",
    "# if we read like std['emma'] it will give an error as there is no index named emma in the series , whereas std['eve'] = 90 created a new index eve in the series ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we can also use slicing to edit multiple values at once .\n",
    "a = pd.Series([12,35,76,23,97,35,64])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a[0:2] = 100",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a[0::2] = 50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a[0:3] = [67,77,87]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fancy indexing also works\n",
    "a[[0,3,4]] = [52,312,653]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series.sample(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# using index label\n",
    "movies_series['Bahubali: The Beginning'] = 'Anushka Shetty'\n",
    "movies_series['Bahubali: The Beginning']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Series with Python Functionalities"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# len(): Returns the number of elements in the Series.\n",
    "print(len(VK) , \"\\n\")  # Example usage\n",
    "\n",
    "# type(): Returns the type of the object.\n",
    "print(type(VK) , \"\\n\")  # Example usage\n",
    "\n",
    "# dir(): Lists all attributes and methods of the object.\n",
    "print(dir(VK) , \"\\n\")  # Example usage\n",
    "\n",
    "# sorted(): Returns a sorted list of the Series values, as it gives output in the form of list , not a series object . hence we don't use sorted instead we use sort_values() method of series .\n",
    "print(sorted(VK) , \"\\n\")  # Example usage\n",
    "\n",
    "# min(): Returns the minimum value in the Series.\n",
    "print(min(VK) , \"\\n\")  # Example usage\n",
    "\n",
    "# max(): Returns the maximum value in the Series.\n",
    "print(max(VK) , \"\\n\")  # Example usage"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "type conversion"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"list : \", list(std),\"\\n\")\n",
    "print(\"Dictionary : \", dict(std),\"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "membership operator\n",
    "- by default, it works indexes ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "'Gladiator' in movies_series",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'Aamir Khan' in movies_series\n",
    "# false because it only searched on indexes ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use .values to search on values .\n",
    "'Aamir Khan' in movies_series.values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "looping\n",
    "- by default it works on values"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in movies_series:\n",
    "  print(i)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To print index use .index :\n",
    "for i in movies_series.index:\n",
    "  print(i)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Arithmetic Operators(Broadcasting)\n",
    "- broadcasting since we are using only one scalar ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "100 + std   # we can use + , - , * , / , ** , %",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Relational Operators\n",
    "VK >= 20"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK != 50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Boolean Indexing on Series"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find no of 50's and 100's scored by kohli\n",
    "VK[VK >= 50].size"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# find number of ducks\n",
    "VK[VK == 0].size"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# find actors who have done more than 4 movies\n",
    "num_movies = movies_series.value_counts()\n",
    "num_movies[num_movies > 4]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Some important Series methods"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`astype()`\n",
    "- it is used to convert the datatype of the series to the desired datatype ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.getsizeof(VK)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK = VK.astype('int16')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sys.getsizeof(VK)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK.dtype",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`between()`\n",
    "- it is used to filter the series values between two given values (inclusive of both the values) ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mask = VK.between(51 , 99)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK[mask].size",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`clip()`\n",
    "- it is used to limit the values in the series to a given range .\n",
    "- values below the lower limit are set to the lower limit , and values above the upper limit are set to the upper limit ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK.clip(lower=50 , upper=100)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`drop_duplicates()`\n",
    "- it is used to remove duplicate values from the series .\n",
    "- here we have the flexibility to delete the first occurrence or the last occurrence of the duplicate values using keep argument , by default it keeps the first occurrence ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp = pd.Series([1,1,2,2,3,3,4,4,5,8,9,9,9,0,1])\n",
    "temp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "temp.drop_duplicates(keep='last')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "movies_series.drop_duplicates()\n",
    "# so now all the actors have one movie and total count is reduced from 517 to 328 ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`duplicated()`\n",
    "- checks if values are duplicated or not ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp.duplicated()\n",
    "# false means not duplicated whereas true means duplicated ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# can also get the no. by adding sum() it basically adds all true so total duplicates .\n",
    "temp.duplicated().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`count() and size`\n",
    "- count function v/s size attribute .\n",
    "    - count function will only count non-nan values .\n",
    "    - size attribute also counts nan values ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t = pd.Series([1,2,3,np.nan,5,6,np.nan,8,np.nan,10])\n",
    "t"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "t.count()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "t.size",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`isnull()`\n",
    "- returns a boolean after checking the complete dataset that whether it's null or not ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "t.isnull()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "t.isnull().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`dropna()`\n",
    "- it is used to drop all the nan values."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "t.dropna()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`fillna()`\n",
    "- it is used to fill the nan values ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "t.fillna(0)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "t.fillna(t.mean())  # so it is filled with the mean of data .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`isin()`\n",
    "- it helps us to check multiple conditions in one go ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# normal way : to check if vk got out on 49 or 99 .\n",
    "VK[(VK == 49) | (VK == 99)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# using isin()\n",
    "VK[VK.isin([49,99])]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`apply()`\n",
    "- it helps us to apply our custom logic ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we want only initial name that too in capital letters of an actor .\n",
    "movies_series.apply(lambda x:x.split()[0].upper())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK.mean()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# now good day when scored more than mean else bad day .\n",
    "VK.apply(lambda x : \"Good Day\" if x > VK.mean() else  \"Bad Day\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`copy()`\n",
    "- it helps us in creating a copy of our original data ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new = VK.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# now if we change match 1 runs of new to 100 .\n",
    "new[1] = 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- now we can see that our original data is also changed in VK , so when we so head() , nail() they don't produce copy they gives us a view of original data that once edited will also effect our original data ."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK[1] = 1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# so instead do this",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_copy = VK.head().copy()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_copy",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_copy[0:3] = 100",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_copy",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VK",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Plotting a `Series`\n",
    "Pandas makes it easy to plot `Series` data using matplotlib (for more details on matplotlib, check out my matplotlib-Guide). Just import matplotlib and call the `plot()` method:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "views.plot()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_series.value_counts().head(15).plot(kind='bar')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\n",
    "s7 = pd.Series(temperatures, name=\"Temperature\")\n",
    "s7.plot()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are *many* options for plotting your data. It is not necessary to list them all here: if you need a particular type of plot (histograms, pie charts, etc.), just look for it in the excellent [Visualization](http://pandas.pydata.org/pandas-docs/stable/visualization.html) section of pandas' documentation, and look at the example code."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Handling time\n",
    "Many datasets have timestamps, and pandas is awesome at manipulating such data:\n",
    "* it can represent periods (such as 2016Q3) and frequencies (such as \"monthly\"),\n",
    "* it can convert periods to actual timestamps, and *vice versa*,\n",
    "* it can resample data and aggregate values any way you like,\n",
    "* it can handle timezones.\n",
    "\n",
    "## Time range\n",
    "Let's start by creating a time series using `pd.date_range()`. This returns a `DatetimeIndex` containing one datetime per hour for 12 hours starting on August 24th 2025 at 5:30pm."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dates = pd.date_range('2025/08/24 5:30pm', periods=12, freq='h')\n",
    "dates"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp_series = pd.Series(temperatures, dates)\n",
    "# type(temp_series)\n",
    "temp_series"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This `DatetimeIndex` may be used as an index in a `Series`:"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this series:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series.plot(kind=\"bar\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "Pandas lets us resample a time series very simply. Just call the `resample()` method and specify a new frequency:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series_freq_2H = temp_series.resample(\"2h\")\n",
    "temp_series_freq_2H"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resampling operation is actually a deferred operation, which is why we did not get a `Series` object, but a `DatetimeIndexResampler` object instead. To actually perform the resampling operation, we can simply call the `mean()` method: Pandas will compute the mean of every pair of consecutive hours:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series_freq_2H = temp_series_freq_2H.mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series_freq_2H.plot(kind=\"bar\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the values have automatically been aggregated into 2-hour periods. If we look at the 6-8pm period, for example, we had a value of `5.1` at 6:30pm, and `6.1` at 7:30pm. After resampling, we just have one value of `5.6`, which is the mean of `5.1` and `6.1`. Rather than computing the mean, we could have used any other aggregation function, for example we can decide to keep the minimum value of each period:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series_freq_2H = temp_series.resample(\"2h\").min()\n",
    "temp_series_freq_2H"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, equivalently, we could use the `apply()` method instead:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series_freq_2H = temp_series.resample(\"2h\").apply(np.min)\n",
    "temp_series_freq_2H"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling and interpolation\n",
    "This was an example of downsampling. We can also upsample (ie. increase the frequency), but this creates holes in our data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series_freq_15min = temp_series.resample(\"15Min\").mean()\n",
    "temp_series_freq_15min.head(n=10) # `head` displays the top n values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution is to fill the gaps by interpolating. We just call the `interpolate()` method. The default is to use linear interpolation, but we can also select another method, such as cubic interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "temp_series_freq_15min = temp_series.resample(\"15Min\").interpolate(method=\"cubic\")\n",
    "temp_series_freq_15min.head(n=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series.plot(label=\"Period: 1 hour\")\n",
    "temp_series_freq_15min.plot(label=\"Period: 15 minutes\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timezones\n",
    "By default datetimes are *naive*: they are not aware of timezones, so 2016-10-30 02:30 might mean October 30th 2016 at 2:30am in Paris or in New York. We can make datetimes timezone *aware* by calling the `tz_localize()` method:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series_ny = temp_series.tz_localize(\"America/New_York\")\n",
    "temp_series_ny"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `-04:00` is now appended to all the datetimes. This means that these datetimes refer to [UTC](https://en.wikipedia.org/wiki/Coordinated_Universal_Time) - 4 hours.\n",
    "\n",
    "We can convert these datetimes to Paris time like this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series_paris = temp_series_ny.tz_convert(\"Europe/Paris\")\n",
    "temp_series_paris"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the UTC offset changes from `+02:00` to `+01:00`: this is because France switches to winter time at 3am that particular night (time goes back to 2am). Notice that 2:30am occurs twice! Let's go back to a naive representation (if you log some data hourly using local time, without storing the timezone, you might get something like this):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series_paris_naive = temp_series_paris.tz_localize(None)\n",
    "temp_series_paris_naive"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `02:30` is really ambiguous. If we try to localize these naive datetimes to the Paris timezone, we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    temp_series_paris_naive.tz_localize(\"Europe/Paris\")\n",
    "except Exception as e:\n",
    "    print(type(e))\n",
    "    print(e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately using the `ambiguous` argument we can tell pandas to infer the right DST (Daylight Saving Time) based on the order of the ambiguous timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp_series_paris_naive.tz_localize(\"Europe/Paris\", ambiguous=\"infer\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periods\n",
    "The `pd.period_range()` function returns a `PeriodIndex` instead of a `DatetimeIndex`. For example, let's get all quarters in 2016 and 2017:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "quarters = pd.period_range('2016Q1', periods=8, freq='Q')\n",
    "quarters"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a number `N` to a `PeriodIndex` shifts the periods by `N` times the `PeriodIndex`'s frequency:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "quarters + 3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `asfreq()` method lets us change the frequency of the `PeriodIndex`. All periods are lengthened or shortened accordingly. For example, let's convert all the quarterly periods to monthly periods (zooming in):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "quarters.asfreq(\"M\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "By default, the `asfreq` zooms on the end of each period. We can tell it to zoom on the start of each period instead:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "quarters.asfreq(\"M\", how=\"start\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can zoom out:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "quarters.asfreq(\"A\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can create a `Series` with a `PeriodIndex`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "quarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters)\n",
    "quarterly_revenue"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "quarterly_revenue.plot(kind=\"line\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert periods to timestamps by calling `to_timestamp`. By default this will give us the first day of each period, but by setting `how` and `freq`, we can get the last hour of each period:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "last_hours = quarterly_revenue.to_timestamp(how=\"end\", freq=\"H\")\n",
    "last_hours"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And back to periods by calling `to_period`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "last_hours.to_period()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also provides many other time-related functions that we recommend you check out in the [documentation](http://pandas.pydata.org/pandas-docs/stable/timeseries.html). To whet your appetite, here is one way to get the last business day of each month in 2016, at 9am:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "months_2016 = pd.period_range(\"2016\", periods=12, freq=\"M\")\n",
    "one_day_after_last_days = months_2016.asfreq(\"D\") + 1\n",
    "last_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay()\n",
    "last_bdays.to_period(\"H\") + 9"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DataFrame` objects\n",
    "A DataFrame object represents a spreadsheet, with cell values, column names and row index labels. You can define expressions to compute columns based on other columns, create pivot-tables, group rows, draw graphs, etc. You can see `DataFrame`s as dictionaries of `Series`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating a `DataFrame`"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Using a `lists`\n",
    "- for this we require a 2D list (list of lists) ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "student_data = [\n",
    "    [\"Alice\", 8.8, 20],\n",
    "    [\"Bob\", 9.2, 19],\n",
    "    [\"Charlie\", 7.9, 31],\n",
    "    [\"David\", 6.8, 9],\n",
    "    [\"Eva\", 8.2, 27]\n",
    "]\n",
    "\n",
    "student = pd.DataFrame(student_data , columns=['Name' , 'CGPA' , 'Package(in LPA)'])\n",
    "student.set_index('Name', inplace=True)  # setting Name as index and making it permanent using inplace = True argument .\n",
    "student     # set_index() is used to set a particular column as index , it will be discussed later in detail ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Using a `Dictionary of Series`\n",
    "- This is the most common way to create a DataFrame.\n",
    "- Each key-value pair in the dictionary corresponds to a column name and a `Series` object.\n",
    "- The `Series` can have different lengths, and missing values will be filled with `NaN`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "people_dict = {\n",
    "    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n",
    "    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n",
    "    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n",
    "    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n",
    "}\n",
    "people = pd.DataFrame(people_dict)\n",
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A few things to note:\n",
    "* the `Series` were automatically aligned based on their index,\n",
    "* missing values are represented as `NaN`,\n",
    "* `Series` names are ignored (the name `\"year\"` was dropped),"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Using `read_csv()`\n",
    "- We can also create a dataframe using read_csv() function of pandas library .\n",
    "- By default it returns a dataframe object .\n",
    "- We can also convert it into a series object using .squeeze() or .iloc[:, col_index] after reading ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "movies_df = pd.read_csv('../DataSets/movies.csv')\n",
    "movies_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ipl_matches = pd.read_csv('../DataSets/ipl-matches.csv')\n",
    "ipl_matches"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DataFrame Attributes and Methods"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Attributes :"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`shape`\n",
    "- it returns a tuple representing the dimensionality of the DataFrame (rows, columns)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.shape",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches.shape",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`dtypes`\n",
    "- it returns the data types of each column in the DataFrame.\n",
    "- here in case of dataframe object we have multiple columns so it returns a series object containing data types of each column ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.dtypes",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches.dtypes",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`index`\n",
    "- it returns the index (row labels) of the DataFrame."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.index",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`columns`\n",
    "- it returns the column names of the DataFrame in the form of Index object."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.columns",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`values`\n",
    "- it returns the underlying data of the DataFrame as a 2D NumPy array that includes all the rows and columns but it does not include index and column names ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.values",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Methods :"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`head() and tail()`\n",
    "- they are used to get a quick overview of the data .\n",
    "- by default they return first/last 5 rows , we can also pass a number to it to get that many rows from top/bottom ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.tail(2)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`sample()`\n",
    "- it is used to get a random sample of items from an axis of object when there is some bias in the data ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.sample()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.sample(3)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`info()`\n",
    "- it is used to get a concise summary of the DataFrame including the index dtype and columns, non-null values and memory usage."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`describe()`\n",
    "- it is used to generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values .\n",
    "- by default it only analyzes numeric columns , if we want to analyze all the columns we can use include='all' argument ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches.describe()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches.describe(include='all')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`isnull()`\n",
    "- it returns a DataFrame of the same shape as the original DataFrame, but with boolean values indicating whether each element is null (NaN) or not.\n",
    "- true means null whereas false means not null .\n",
    "- we can also get the count of null values in each column by adding sum() at the end .\n",
    "- we can also use isna() method instead of isnull() as both are same .\n",
    "- we can also use notnull() or notna() method to check non-null values ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.isnull()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.isnull().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`duplicated()`\n",
    "- it checks if rows are duplicated or not ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.duplicated().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches.duplicated().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`rename()`\n",
    "- it is used to rename the index (row labels) or columns of the DataFrame.\n",
    "- we can use index= {} or columns= {} argument to specify whether we want to rename index or columns .\n",
    "- by default it returns a new DataFrame with the updated names, leaving the original DataFrame unchanged. To make the changes permanent, you can use the inplace=True argument."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student.rename(columns = {'Name' : 'Student Name' , 'CGPA' : 'GPA'} )",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- original dataframe is not changed .\n",
    "- to make it permanent use inplace = True argument ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student.rename(columns = {'Name' : 'Student Name' , 'CGPA' : 'GPA'} , inplace= True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Math Methods"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`sum() , mean() , median() , mode() , std() , var()`\n",
    "- by default, they work on columns (axis=0) , we can also use axis=1 argument to work on rows .\n",
    "- they only consider numeric columns and ignore non-numeric columns ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- sum() :\n",
    "    - for numeric columns it returns the sum of each column .\n",
    "    - for non-numeric columns it returns the concatenation of all values in that column .\n",
    "    - if we want to consider only numeric columns we can use numeric_only=True argument ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sum :\n",
    "student.sum()   # sum of each column"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- mean() :\n",
    "    - it returns the mean of each numeric column .\n",
    "    - if we want to consider only numeric columns we can use numeric_only=True argument ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student.mean(axis=1 , numeric_only=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student.var(numeric_only=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`min() and max()`\n",
    "- they return the minimum and maximum value of each column respectively.\n",
    "- if we want to consider only numeric columns we can use numeric_only=True argument ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student.min()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Selecting cols from a DataFrame"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Single column :\n",
    "    - we can access a single column using either the column name as an attribute or by using the column name as a key in square brackets.\n",
    "    - type of the returned object is Series ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.title_x",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "m = movies_df['title_x']\n",
    "m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(m)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches.WinningTeam",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Multiple columns :\n",
    "    - we can access multiple columns by passing a list of column names to the DataFrame constructor.\n",
    "    - type of the returned object is DataFrame ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n = movies_df[['title_x' , 'runtime' , 'year_of_release' , 'actors']]   # selecting multiple columns by passing a list of column names to the DataFrame constructor .\n",
    "n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Selecting rows from a DataFrame using **iloc** and **loc**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1. **iloc** - searches using index positions :\n",
    "   - it is used for integer based indexing , it stands for integer location .\n",
    "   - it accepts only integer values or boolean values .\n",
    "   - it does not accept index labels or column names .\n",
    "   - it works on both rows and columns .\n",
    "   - syntax : df.iloc[row_index , column_index]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Single row :\n",
    "    - we can access a single row using either the index position with iloc or the index label with loc .\n",
    "    - both return a Series object representing the row."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.iloc[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(movies_df.iloc[0])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- multiple rows :\n",
    "    - we can access multiple rows by passing a list of index positions to iloc or a list of index labels to loc .\n",
    "    - both return a DataFrame object representing the selected rows."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.iloc[0:15:3]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fancy indexing also works\n",
    "movies_df.iloc[[0,3,5,7,10,12]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2. **loc** - searches using index labels :\n",
    "   - it is used for label based indexing , it stands for location .\n",
    "   - it accepts only index labels or column names .\n",
    "   - it does not accept integer values or boolean values .\n",
    "   - it works on both rows and columns .\n",
    "   - syntax : df.loc[row_label , column_name]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student.loc['Alice']",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student.loc['Alice':'David']   # it includes the last index label also unlike iloc where last index position is excluded .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- in case of loc , the last index label is included whereas in case of iloc last index position is excluded ."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- not supported :"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student.loc['alice':'Eva':2]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- When slicing a DataFrame by labels, you can only specify the start and stop values. Label slicing doesnâ€™t support a step. To skip rows, first select the label range with .loc and then apply a step-based slice with .iloc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use .loc to slice by labels then .iloc to take every 2nd row\n",
    "result = student.loc['Alice':'Charlie'].iloc[::2]\n",
    "print(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fancy indexing also works\n",
    "student.loc[['Alice', 'Eva', 'David']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- since in students we have custom index but still we can use integer based indexing using iloc , because pandas internally maintains a default integer index for all DataFrames ."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = student.iloc[2]\n",
    "print(type(x))\n",
    "x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "student.iloc[0:3]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Selecting both rows and cols"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.iloc[0:3,0:3]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.loc[0:2,'title_x':'poster_path']",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- these row index are default integer index , if we have custom index then we can use that also using loc , but then too we can use integer based indexing using iloc  as pandas internally maintains a default integer index for all DataFrames ."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtering a DataFrame"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. **Find all the winners of ipl seasons**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask = ipl_matches['MatchNumber'] == 'Final'\n",
    "new_df = ipl_matches[mask]\n",
    "new_df[['Season', 'WinningTeam']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Directly\n",
    "ipl_matches[ipl_matches['MatchNumber'] == 'Final'][['Season', 'WinningTeam']]\n",
    "# here we used boolean indexing to filter the dataframe and then selected only two columns from it using fancy indexing ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. **How many super over finishes have occured**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ipl_matches[ipl_matches['SuperOver'] == 'Y'].shape[0]\n",
    "# We have used shape attribute to get the number of rows in the filtered dataframe ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# other method\n",
    "x = ipl_matches['SuperOver'] == 'Y'\n",
    "x.sum()\n",
    "# In this method we are getting a boolean series and then adding it , so total true will be the count of super over matches ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. **How many matches has csk won in kolkata**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ipl_matches[(ipl_matches['WinningTeam'] == 'Chennai Super Kings' ) & (ipl_matches['City'] == 'Kolkata') ].shape[0]\n",
    "# we used & instead of && because we are dealing with binary values (series of true and false) here ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. **Toss winner is match winner in percentage**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ipl_matches[ipl_matches['TossWinner'] == ipl_matches['WinningTeam']].shape[0] / ipl_matches.shape[0] * 100\n",
    "# we used shape attribute to get the number of rows in the filtered dataframe and total dataframe , then calculated the percentage ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5. **Movies with rating higher than 8 and votes>10000**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df[ (movies_df['imdb_rating'] > 8) & (movies_df['imdb_votes'] > 10000) ][['title_x', 'imdb_rating' , 'imdb_votes']]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# to get the count :\n",
    "\n",
    "# method 1 -\n",
    "m1 = movies_df[ (movies_df['imdb_rating'] > 8) & (movies_df['imdb_votes'] > 10000) ].shape[0]\n",
    "print(m1 , \"\\n\")\n",
    "\n",
    "# method 2 -\n",
    "m2 = (movies_df['imdb_rating'] > 8) & (movies_df['imdb_votes'] > 10000)\n",
    "m2.sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6. **Action movies with rating higher than 7.5**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "movies_df[ (movies_df['genres'] == 'Action') & movies_df['imdb_rating'] > 7.5 ]\n",
    "# Not working because genres column is made up of multiple genres that is why it is unable to fetch Action alone ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now we have to split the genres with '|' to extract rows with action genres .\n",
    "\n",
    "# movies_df['genres'].split('|')\n",
    "\n",
    "# But series has no method named split() ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# So , firstly we'll convert genres column into string and then apply split .\n",
    "mask1 =  movies_df['genres'].str.split('|').apply(lambda x : 'Action' in x)\n",
    "mask2 = movies_df['imdb_rating']  > 7.5\n",
    "movies_df[mask1 & mask2][ ['original_title' , 'genres' , 'imdb_rating'] ]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Can also use contains function after converting genres to string , it will check the presence of Action in the string .\n",
    "mask1 =  movies_df['genres'].str.contains('Action')\n",
    "mask2 = movies_df['imdb_rating']  > 7.5\n",
    "movies_df[mask1 & mask2][ ['original_title' , 'genres' , 'imdb_rating'] ]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7. **write a function that can return the track record of 2 teams(rcb v/s csk) against each other**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "ipl_matches[\n",
    "    (ipl_matches['Team1'].isin(['Royal Challengers Bangalore', 'Chennai Super Kings'])) &\n",
    "    (ipl_matches['Team2'].isin(['Royal Challengers Bangalore', 'Chennai Super Kings']))\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding new cols"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. completely new\n",
    "-"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "movies_df['Country'] = 'India'\n",
    "movies_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. **From existing ones**\n",
    "-"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# dropna() is used to remove nan values from the table .\n",
    "# doing it like this will drop each row having a null not only actor column .\n",
    "movies_df.dropna(inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "movies_df['lead actor'] = movies_df['actors'].str.split('|').apply(lambda x:x[0])\n",
    "movies_df['lead actor']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Important DataFrame Functions"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`astype()` :\n",
    "- it is used to change the datatype of a column to a specific type ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches['ID'] = ipl_matches['ID'].astype('int32')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# size reduced from 148.6+ KB to 144.9+ KB",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `category` datatype in pandas is used for columns with a fixed number of possible values (categories). It saves memory and speeds up operations. Useful for representing categorical data like gender, grades, or labels. Supports ordering and custom category names."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ipl_matches['Season'] = ipl_matches['Season'].astype('category')\n",
    "ipl_matches['Team1'] = ipl_matches['Team1'].astype('category')\n",
    "ipl_matches['Team2'] = ipl_matches['Team2'].astype('category')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# further size reduced to 127.4+ KB .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### More Important Functions"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `value_counts()`\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to get a count of unique values in a column .\n",
    "- it returns a series object containing unique values as index and their counts as values ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = pd.Series([1,1,1,2,2,3])\n",
    "a.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "marks = pd.DataFrame([\n",
    "    [100,80,10],\n",
    "    [90,70,7],\n",
    "    [120,100,14],\n",
    "    [80,70,14],\n",
    "    [80,70,14]\n",
    "],columns=['iq','marks','package'])\n",
    "\n",
    "marks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = marks.value_counts()\n",
    "x\n",
    "# here it considers all the columns to find unique rows and their counts ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(x)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ipl_matches = pd.read_csv('../DataSets/ipl-matches.csv')\n",
    "movies_df = pd.read_csv('../DataSets/movies.csv')\n",
    "batsman_runs = pd.read_csv('../DataSets/batsman_runs_ipl.csv')\n",
    "diabetes = pd.read_csv('../DataSets/diabetes.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. **Find which player has won most potm -> in finals and qualifiers**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ipl_matches[~ipl_matches['MatchNumber'].str.isdigit()]['Player_of_Match'].value_counts().head(1)\n",
    "# here we used ~ to negate the boolean series returned by isdigit() function to filter out only those rows where Player_of_Match is not a digit , and since it is a string function so we used str before it ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. **Toss decision plot**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipl_matches['TossDecision'].value_counts().plot(kind = 'pie')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. **How many matches each team has played**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "( ipl_matches['Team1'].value_counts() + ipl_matches['Team2'].value_counts()).sort_values(ascending=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# here we got the count of matches played by each team by adding the count of Team1 and Team2 columns and since it returned a series so we used sort_values() method to sort it in descending order .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# We can observe that value_counts() function is very important and useful in data analysis .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `sort_values`\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to sort the dataframe based on the values of a particular column .\n",
    "- by default it sorts in ascending order , we can use ascending=False argument to sort in descending order ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = pd.Series([12,14,1,56,89])\n",
    "x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x.sort_values(ascending=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "movies_df['title_x'].sort_values()\n",
    "# but it returns a series object ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.sort_values('title_x')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# so we'll do it like this to get a dataframe object .\n",
    "movies_df.sort_values(by='title_x', ascending=False)\n",
    "# by attribute we can specify the column name on which we want to sort the dataframe ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "movies_df.sort_values(by=['year_of_release', 'imdb_rating'], ascending=[False, True])\n",
    "# Sort the DataFrame first by 'year_of_release' in descending order.\n",
    "# If there are ties (same year), then sort those rows by 'imdb_rating' in ascending order.\n",
    "# This is called lexicographical sorting and avoids conflicts by using the order of columns."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Here we can also pass a list of column names to sort by multiple columns at once and also skip the use of by attribute .\n",
    "movies_df.sort_values(['year_of_release', 'imdb_rating' , 'title_x' ],ascending=[True,False , True])\n",
    "# Here we sorted the dataframe first by year_of_release in ascending order , and when there are ties (same year) then those rows are sorted by imdb_rating in descending order and if there are still ties then those rows are sorted by title_x in ascending order ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "students = pd.DataFrame(\n",
    "    {\n",
    "        'name':['nitish','ankit','rupesh',np.nan,'mrityunjay',np.nan,'rishabh',np.nan,'aditya',np.nan],\n",
    "        'college':['bit','iit','vit',np.nan,np.nan,'vlsi','ssit',np.nan,np.nan,'git'],\n",
    "        'branch':['eee','it','cse',np.nan,'me','ce','civ','cse','bio',np.nan],\n",
    "        'cgpa':[6.66,8.25,6.41,np.nan,5.6,9.0,7.4,10,7.4,np.nan],\n",
    "        'package':[4,5,6,np.nan,6,7,8,9,np.nan,np.nan]\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "students"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "students.sort_values(by= 'name')\n",
    "# by default, it places nan values at the end , we can use na_position='first' argument to place them at the beginning ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "students.sort_values('name',na_position='first',ascending=False,inplace=True)\n",
    "students"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `rank()`\n",
    "\n",
    "- only works on series objects .\n",
    "- it is used to get the rank of each value in a series .\n",
    "- by default it assigns the average rank to the tied values , we can use method='min' argument to assign the minimum rank to the tied values .\n",
    "- by default it ranks in ascending order , we can use ascending=False argument to rank in descending order .\n",
    "- when the value is same for multiple rows , it assigns the same rank to all those rows and skips the next ranks accordingly . ex: if two rows have same value and they are ranked 2nd , then the next rank will be 4th ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "batsman_runs",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batsman_runs['batting_rank'] = batsman_runs['batsman_run'].rank(ascending=False)\n",
    "batsman_runs.sort_values('batting_rank')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `sort_index()`\n",
    "\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to sort the dataframe based on the index (row labels) ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "marks = {\n",
    "    'maths':67,\n",
    "    'english':57,\n",
    "    'science':89,\n",
    "    'hindi':100\n",
    "}\n",
    "\n",
    "marks_series = pd.Series(marks)\n",
    "marks_series"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "marks_series.sort_index(ascending=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.sort_index(ascending=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `set_index()`\n",
    "- works only on dataframe objects .\n",
    "- it is used to set a particular column as index (row labels) of the dataframe .\n",
    "- by default, it returns a new dataframe with the updated index , leaving the original dataframe unchanged .\n",
    "- if we want to set multiple columns as index , we can pass a list of column names to it ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batsman_runs.set_index('batter',inplace=True)\n",
    "batsman_runs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `reset_index()`\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to reset the index (row labels) of the dataframe to default integer index ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batsman_runs.reset_index(inplace=True)\n",
    "batsman_runs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "batsman_runs.set_index('batter',inplace=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# how to replace existing index without loosing .\n",
    "# means if we set batter as index and then want to set batting_rank as index and if we do it directly like batsman.set_index('batting_rank') then we will loose batter index .\n",
    "# so for that we will first reset the index to default integer index by using reset_index() and then set batting_rank as index .\n",
    "batsman_runs.reset_index().set_index('batting_rank')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# series to dataframe using reset_index\n",
    "marks_series"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = marks_series.reset_index()\n",
    "x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "type(x)\n",
    "# so it converted series to dataframe ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `rename()`\n",
    "\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to rename the index (row labels) or columns of the dataframe ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df.set_index('title_x',inplace=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "movies_df.rename(columns={'imdb_id':'imdb', 'poster_path':'link'},inplace=True)\n",
    "movies_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We can also rename index (row labels) using rename() method .\n",
    "movies_df.rename(index={'Uri: The Surgical Strike':'uri', 'Manikarnika: The Queen of Jhansi':'Manikarnika'},inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "movies_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `unique()`\n",
    "- works only on series objects .\n",
    "- it is used to get the unique values in a column .\n",
    "- unique also returns np.nan as a unique value ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp = pd.Series([1,1,2,2,3,3,4,4,5,5,np.nan,np.nan])\n",
    "print(temp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "temp.unique()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(temp.unique())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "temp.nunique()      # to get the count of unique values in the series but it does not consider np.nan as a unique value by default .",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `nunique()`\n",
    "- works only on series objects .\n",
    "- it is used to get the count of unique values in a column .\n",
    "- it does not consider np.nan as a unique value by default , we can use dropna=False argument to consider np.nan as a unique value ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print( len(ipl_matches['Season'].unique()) )\n",
    "print(ipl_matches['Season'].unique().size)\n",
    "print(ipl_matches['Season'].unique().shape[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `isnull()`\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to check for null values in a column or dataframe .\n",
    "- it returns a boolean series or dataframe indicating whether each value is null (NaN) or not ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students['name'][students['name'].isnull()]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `notnull()`\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to check for non-null values in a column or dataframe .\n",
    "- it returns a boolean series or dataframe indicating whether each value is not null (not NaN) or not ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students['name'][students['name'].notnull()]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `hasnans`\n",
    "- works only on series objects .\n",
    "- it is used to check if there are any null values in a column or dataframe .\n",
    "- it returns a boolean value indicating whether there are any null values or not ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students['name'].hasnans",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- working with DataFrames"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students.isnull()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students.notnull()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# students.hasnans\n",
    "# does not work for dataframe object , it only works for series object ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `dropna()`\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to drop the null values from a column or dataframe .\n",
    "- thresh argument is used to specify the minimum number of non-null values required to keep a row or column .\n",
    "- subset argument is used to specify a list of columns to consider for dropping null values ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students['name'].dropna()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "students.dropna()\n",
    "# by default, it drops the rows having any null value (because how = 'any' is default), we can use how='all' argument to drop the rows having all null values ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students.dropna(how='all')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students.dropna(subset=['name'])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "students.dropna(subset=['name','college'])\n",
    "# here it drops the rows having null values in either name or college column ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `fillna()`\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to fill the null values in a column or dataframe with a specific value or method .\n",
    "- method argument is used to specify the method to use for filling null values . it can be 'ffill' (forward fill) or 'bfill' (backward fill) .\n",
    "- limit argument is used to specify the maximum number of consecutive null values to fill ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students['name'].fillna('unknown').hasnans",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students['package'].fillna(students['package'].mean())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students['name'].fillna(method='bfill')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Note: passing `method` to `fillna` is deprecated. Prefer `.bfill()` and ffill()`.\n",
    "students['name'].bfill()\n",
    "# bfill is used to fill the null values with before non-null value and"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `drop_duplicates()`\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to drop the duplicate values from a column or dataframe .\n",
    "- by default, it keeps the first occurrence of the duplicate values and drops the rest , we can use keep='last' argument to keep the last occurrence of the duplicate values and drop the rest .\n",
    "- subset argument is used to specify a list of columns to consider for dropping duplicate values ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "marks = pd.DataFrame([\n",
    "    [100,80,10],\n",
    "    [90,70,7],\n",
    "    [120,100,14],\n",
    "    [80,70,14],\n",
    "    [80,70,14]\n",
    "],columns=['iq','marks','package'])\n",
    "\n",
    "marks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# so duplicated tells us which rows are duplicate and drop_duplicates() drops those duplicate rows .\n",
    "print ( marks.duplicated() )\n",
    "print(\"\\n\" , marks.duplicated().sum() )\n",
    "# it provides a boolean series indicating whether each row is a duplicate or not and also the count of duplicate rows in the second print statement ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp = pd.Series([1,1,1,2,3,3,4,4])\n",
    "temp.drop_duplicates()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "marks.drop_duplicates(keep='last')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* find the last match played by virat kohli in Delhi ??"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ipl_matches['all_players'] = ipl_matches['Team1Players'] + ipl_matches['Team2Players']\n",
    "ipl_matches.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def did_kohli_play(players_list):\n",
    "  return 'V Kohli' in players_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ipl_matches['did_kohli_play'] = ipl_matches['all_players'].apply(did_kohli_play)\n",
    "ipl_matches[(ipl_matches['City'] == 'Delhi') & (ipl_matches['did_kohli_play'] == True)].drop_duplicates(subset=['City','did_kohli_play'],keep='first')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `drop()`\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to drop a specific row or column from a dataframe .\n",
    "- axis argument is used to specify whether to drop a row (axis=0) or a column (axis=1) ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp = pd.Series([10,2,3,16,45,78,10])\n",
    "temp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "temp.drop(index=[0,6])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students.drop(columns=['branch','cgpa'])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students.drop(index = [0,3,8])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# it also works fine with custom index also .\n",
    "students.set_index('name',inplace=True)\n",
    "students"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "students.drop(index=['nitish','aditya'])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### `apply()`\n",
    "- works on both series and dataframe objects .\n",
    "- it is used to apply a specific function to each element of a column or dataframe .\n",
    "- axis argument is used to specify whether to apply the function to rows (axis=0) or columns (axis=1) .\n",
    "- if we want to pass multiple arguments to the function , we can use args argument to pass a tuple of arguments .\n",
    "- we can also use lambda function to apply a specific function to each element of a column or dataframe ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp = pd.Series([10,20,30,40,50])\n",
    "temp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sigmoid(value):\n",
    "  return 1/1+np.exp(-value)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "temp.apply(sigmoid)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "points_df = pd.DataFrame(\n",
    "    {\n",
    "        '1st point':[(3,4),(-6,5),(0,0),(-10,1),(4,5)],\n",
    "        '2nd point':[(-3,4),(0,0),(2,2),(10,10),(1,1)]\n",
    "    }\n",
    ")\n",
    "\n",
    "points_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def euclidean(row):\n",
    "  pt_A = row['1st point']\n",
    "  pt_B = row['2nd point']\n",
    "\n",
    "  return ((pt_A[0] - pt_B[0])**2 + (pt_A[1] - pt_B[1])**2)**0.5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "points_df['distance'] = points_df.apply(euclidean,axis=1)\n",
    "points_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access columns pretty much as you would expect. They are returned as `Series` objects:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people[\"birthyear\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get multiple columns at once:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people[[\"birthyear\", \"hobby\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass a list of columns and/or index row labels to the `DataFrame` constructor, it will guarantee that these columns and/or rows will exist, in that order, and no other column/row will exist. For example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d2 = pd.DataFrame(\n",
    "        people_dict,\n",
    "        columns=[\"birthyear\", \"weight\", \"height\"],\n",
    "        index=[\"bob\", \"alice\", \"eugene\"]\n",
    "     )\n",
    "d2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convenient way to create a `DataFrame` is to pass all the values to the constructor as an `ndarray`, or a list of lists, and specify the column names and row index labels separately:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "values = [\n",
    "            [1985, np.nan, \"Biking\",   68],\n",
    "            [1984, 3,      \"Dancing\",  83],\n",
    "            [1992, 0,      np.nan,    112]\n",
    "         ]\n",
    "d3 = pd.DataFrame(\n",
    "        values,\n",
    "        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n",
    "        index=[\"alice\", \"bob\", \"charles\"]\n",
    "     )\n",
    "d3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify missing values, you can either use `np.nan` or NumPy's masked arrays:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "masked_array = np.ma.asarray(values, dtype=np.object)\n",
    "masked_array[(0, 2), (1, 2)] = np.ma.masked\n",
    "d3 = pd.DataFrame(\n",
    "        masked_array,\n",
    "        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n",
    "        index=[\"alice\", \"bob\", \"charles\"]\n",
    "     )\n",
    "d3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of an `ndarray`, you can also pass a `DataFrame` object:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d4 = pd.DataFrame(\n",
    "         d3,\n",
    "         columns=[\"hobby\", \"children\"],\n",
    "         index=[\"alice\", \"bob\"]\n",
    "     )\n",
    "d4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create a `DataFrame` with a dictionary (or list) of dictionaries (or list):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people = pd.DataFrame({\n",
    "    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992},\n",
    "    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"},\n",
    "    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112},\n",
    "    \"children\": {\"bob\": 3, \"charles\": 0}\n",
    "})\n",
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-indexing\n",
    "If all columns are tuples of the same size, then they are understood as a multi-index. The same goes for row index labels. For example:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d5 = pd.DataFrame(\n",
    "  {\n",
    "    (\"public\", \"birthyear\"):\n",
    "        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\n",
    "    (\"public\", \"hobby\"):\n",
    "        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n",
    "    (\"private\", \"weight\"):\n",
    "        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n",
    "    (\"private\", \"children\"):\n",
    "        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n",
    "  }\n",
    ")\n",
    "d5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now get a `DataFrame` containing all the `\"public\"` columns very simply:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d5[\"public\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d5[\"public\", \"hobby\"]  # Same result as d5[\"public\"][\"hobby\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping a level\n",
    "Let's look at `d5` again:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two levels of columns, and two levels of indices. We can drop a column level by calling `droplevel()` (the same goes for indices):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d5.columns = d5.columns.droplevel(level = 0)\n",
    "d5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposing\n",
    "You can swap columns and indices using the `T` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d6 = d5.T\n",
    "d6"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking and unstacking levels\n",
    "Calling the `stack()` method will push the lowest column level after the lowest index:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d7 = d6.stack()\n",
    "d7"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many `NaN` values appeared. This makes sense because many new combinations did not exist before (eg. there was no `bob` in `London`).\n",
    "\n",
    "Calling `unstack()` will do the reverse, once again creating many `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d8 = d7.unstack()\n",
    "d8"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call `unstack` again, we end up with a `Series` object:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d9 = d8.unstack()\n",
    "d9"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stack()` and `unstack()` methods let you select the `level` to stack/unstack. You can even stack/unstack multiple levels at once:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "d10 = d9.unstack(level = (0,1))\n",
    "d10"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most methods return modified copies\n",
    "As you may have noticed, the `stack()` and `unstack()` methods do not modify the object they apply to. Instead, they work on a copy and return that copy. This is true of most methods in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing rows\n",
    "Let's go back to the `people` `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `loc` attribute lets you access rows instead of columns. The result is a `Series` object in which the `DataFrame`'s column names are mapped to row index labels:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.loc[\"charles\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access rows by integer location using the `iloc` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.iloc[2]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get a slice of rows, and this returns a `DataFrame` object:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.iloc[1:3]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can pass a boolean array to get the matching rows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people[np.array([True, False, True])]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is most useful when combined with boolean expressions:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people[people[\"birthyear\"] < 1990]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and removing columns\n",
    "You can generally treat `DataFrame` objects like dictionaries of `Series`, so the following work fine:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people[\"age\"] = 2018 - people[\"birthyear\"]  # adds a new column \"age\"\n",
    "people[\"over 30\"] = people[\"age\"] > 30      # adds another column \"over 30\"\n",
    "birthyears = people.pop(\"birthyear\")\n",
    "del people[\"children\"]\n",
    "\n",
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "birthyears"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you add a new colum, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice is missing, eugene is ignored\n",
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the `insert()` method:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.insert(1, \"height\", [172, 181, 185])\n",
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning new columns\n",
    "You can also create new columns by calling the `assign()` method. Note that this returns a new `DataFrame` object, the original is not modified:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.assign(\n",
    "    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n",
    "    has_pets = people[\"pets\"] > 0\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you cannot access columns created within the same assignment:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    people.assign(\n",
    "        body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n",
    "        overweight = people[\"body_mass_index\"] > 25\n",
    "    )\n",
    "except KeyError as e:\n",
    "    print(\"Key error:\", e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to split this assignment in two consecutive assignments:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d6 = people.assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\n",
    "d6.assign(overweight = d6[\"body_mass_index\"] > 25)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having to create a temporary variable `d6` is not very convenient. You may want to just chain the assigment calls, but it does not work because the `people` object is not actually modified by the first assignment:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    (people\n",
    "         .assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\n",
    "         .assign(overweight = people[\"body_mass_index\"] > 25)\n",
    "    )\n",
    "except KeyError as e:\n",
    "    print(\"Key error:\", e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But fear not, there is a simple solution. You can pass a function to the `assign()` method (typically a `lambda` function), and this function will be called with the `DataFrame` as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "(people\n",
    "     .assign(body_mass_index = lambda df: df[\"weight\"] / (df[\"height\"] / 100) ** 2)\n",
    "     .assign(overweight = lambda df: df[\"body_mass_index\"] > 25)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem solved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating an expression\n",
    "A great feature supported by pandas is expression evaluation. This relies on the `numexpr` library which must be installed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.eval(\"weight / (height/100) ** 2 > 25\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment expressions are also supported. Let's set `inplace=True` to directly modify the `DataFrame` rather than getting a modified copy:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.eval(\"body_mass_index = weight / (height/100) ** 2\", inplace=True)\n",
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a local or global variable in an expression by prefixing it with `'@'`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "overweight_threshold = 30\n",
    "people.eval(\"overweight = body_mass_index > @overweight_threshold\", inplace=True)\n",
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying a `DataFrame`\n",
    "The `query()` method lets you filter a `DataFrame` based on a query expression:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.query(\"age > 30 and pets == 0\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting a `DataFrame`\n",
    "You can sort a `DataFrame` by calling its `sort_index` method. By default it sorts the rows by their index label, in ascending order, but let's reverse the order:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.sort_index(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `sort_index` returned a sorted *copy* of the `DataFrame`. To modify `people` directly, we can set the `inplace` argument to `True`. Also, we can sort the columns instead of the rows by setting `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.sort_index(axis=1, inplace=True)\n",
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sort the `DataFrame` by the values instead of the labels, we can use `sort_values` and specify the column to sort by:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.sort_values(by=\"age\", inplace=True)\n",
    "people"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a `DataFrame`\n",
    "Just like for `Series`, pandas makes it easy to draw nice graphs based on a `DataFrame`.\n",
    "\n",
    "For example, it is trivial to create a line plot from a `DataFrame`'s data by calling its `plot` method:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "people.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass extra arguments supported by matplotlib's functions. For example, we can create scatterplot and pass it a list of sizes using the `s` argument of matplotlib's `scatter()` function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "people.plot(kind = \"scatter\", x = \"height\", y = \"weight\", s=[40, 120, 200])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there are way too many options to list here: the best option is to scroll through the [Visualization](http://pandas.pydata.org/pandas-docs/stable/visualization.html) page in pandas' documentation, find the plot you are interested in and look at the example code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on `DataFrame`s\n",
    "Although `DataFrame`s do not try to mimick NumPy arrays, there are a few similarities. Let's create a `DataFrame` to demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\n",
    "grades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\n",
    "grades"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply NumPy mathematical functions on a `DataFrame`: the function is applied to all values:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.sqrt(grades)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, adding a single value to a `DataFrame` will add that value to all elements in the `DataFrame`. This is called *broadcasting*:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grades + 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the same is true for all other binary operations, including arithmetic (`*`,`/`,`**`...) and conditional (`>`, `==`...) operations:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grades >= 5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation operations, such as computing the `max`, the `sum` or the `mean` of a `DataFrame`, apply to each column, and you get back a `Series` object:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grades.mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `all` method is also an aggregation operation: it checks whether all values are `True` or not. Let's see during which months all students got a grade greater than `5`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "(grades > 5).all()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these functions take an optional `axis` parameter which lets you specify along which axis of the `DataFrame` you want the operation executed. The default is `axis=0`, meaning that the operation is executed vertically (on each column). You can set `axis=1` to execute the operation horizontally (on each row). For example, let's find out which students had all grades greater than `5`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "(grades > 5).all(axis = 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `any` method returns `True` if any value is True. Let's see who got at least one grade 10:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "(grades == 10).any(axis = 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you add a `Series` object to a `DataFrame` (or execute any other binary operation), pandas attempts to broadcast the operation to all *rows* in the `DataFrame`. This only works if the `Series` has the same size as the `DataFrame`s rows. For example, let's subtract the `mean` of the `DataFrame` (a `Series` object) from the `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grades - grades.mean()  # equivalent to: grades - [7.75, 8.75, 7.50]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subtracted `7.75` from all September grades, `8.75` from October grades and `7.50` from November grades. It is equivalent to subtracting this `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to subtract the global mean from every grade, here is one way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "grades - grades.values.mean() # subtracts the global mean (8.00) from all grades"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic alignment\n",
    "Similar to `Series`, when operating on multiple `DataFrame`s, pandas automatically aligns them by row index label, but also by column names. Let's create a `DataFrame` with bonus points for each person from October to December:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]])\n",
    "bonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\",\"colin\", \"darwin\", \"charles\"])\n",
    "bonus_points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "grades + bonus_points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the addition worked in some cases but way too many elements are now empty. That's because when aligning the `DataFrame`s, some columns and rows were only present on one side, and thus they were considered missing on the other side (`NaN`). Then adding `NaN` to a number results in `NaN`, hence the result.\n",
    "\n",
    "## Handling missing data\n",
    "Dealing with missing data is a frequent task when working with real life data. Pandas offers a few tools to handle missing data.\n",
    " \n",
    "Let's try to fix the problem above. For example, we can decide that missing data should result in a zero, instead of `NaN`. We can replace all `NaN` values by a any value using the `fillna()` method:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "(grades + bonus_points).fillna(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit unfair that we're setting grades to zero in September, though. Perhaps we should decide that missing grades are missing grades, but missing bonus points should be replaced by zeros:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fixed_bonus_points = bonus_points.fillna(0)\n",
    "fixed_bonus_points.insert(0, \"sep\", 0)\n",
    "fixed_bonus_points.loc[\"alice\"] = 0\n",
    "grades + fixed_bonus_points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better: although we made up some data, we have not been too unfair.\n",
    "\n",
    "Another way to handle missing data is to interpolate. Let's look at the `bonus_points` `DataFrame` again:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bonus_points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the `interpolate` method. By default, it interpolates vertically (`axis=0`), so let's tell it to interpolate horizontally (`axis=1`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bonus_points.interpolate(axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bob had 0 bonus points in October, and 2 in December. When we interpolate for November, we get the mean: 1 bonus point. Colin had 1 bonus point in November, but we do not know how many bonus points he had in September, so we cannot interpolate, this is why there is still a missing value in October after interpolation. To fix this, we can set the September bonus points to 0 before interpolation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "better_bonus_points = bonus_points.copy()\n",
    "better_bonus_points.insert(0, \"sep\", 0)\n",
    "better_bonus_points.loc[\"alice\"] = 0\n",
    "better_bonus_points = better_bonus_points.interpolate(axis=1)\n",
    "better_bonus_points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have reasonable bonus points everywhere. Let's find out the final grades:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grades + better_bonus_points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is slightly annoying that the September column ends up on the right. This is because the `DataFrame`s we are adding do not have the exact same columns (the `grades` `DataFrame` is missing the `\"dec\"` column), so to make things predictable, pandas orders the final columns alphabetically. To fix this, we can simply add the missing column before adding:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "grades[\"dec\"] = np.nan\n",
    "final_grades = grades + better_bonus_points\n",
    "final_grades"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's not much we can do about December and Colin: it's bad enough that we are making up bonus points, but we can't reasonably make up grades (well I guess some teachers probably do). So let's call the `dropna()` method to get rid of rows that are full of `NaN`s:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "final_grades_clean = final_grades.dropna(how=\"all\")\n",
    "final_grades_clean"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove columns that are full of `NaN`s by setting the `axis` argument to `1`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "final_grades_clean = final_grades_clean.dropna(axis=1, how=\"all\")\n",
    "final_grades_clean"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating with `groupby`\n",
    "Similar to the SQL language, pandas allows grouping your data into groups to run calculations over each group.\n",
    "\n",
    "First, let's add some extra data about each person so we can group them, and let's go back to the `final_grades` `DataFrame` so we can see how `NaN` values are handled:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "final_grades[\"hobby\"] = [\"Biking\", \"Dancing\", np.nan, \"Dancing\", \"Biking\"]\n",
    "final_grades"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's group data in this `DataFrame` by hobby:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grouped_grades = final_grades.groupby(\"hobby\")\n",
    "grouped_grades"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to compute the average grade per hobby:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grouped_grades.mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy! Note that the `NaN` values have simply been skipped when computing the means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot tables\n",
    "Pandas supports spreadsheet-like [pivot tables](https://en.wikipedia.org/wiki/Pivot_table) that allow quick data summarization. To illustrate this, let's create a simple `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bonus_points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "more_grades = final_grades_clean.stack().reset_index()\n",
    "more_grades.columns = [\"name\", \"month\", \"grade\"]\n",
    "more_grades[\"bonus\"] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0]\n",
    "more_grades"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the `pd.pivot_table()` function for this `DataFrame`, asking to group by the `name` column. By default, `pivot_table()` computes the mean of each numeric column:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.pivot_table(more_grades, index=\"name\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the aggregation function by setting the `aggfunc` argument, and we can also specify the list of columns whose values will be aggregated:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.pivot_table(more_grades, index=\"name\", values=[\"grade\",\"bonus\"], aggfunc=np.max)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the `columns` to aggregate over horizontally, and request the grand totals for each row and column by setting `margins=True`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.pivot_table(more_grades, index=\"name\", values=\"grade\", columns=\"month\", margins=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can specify multiple index or column names, and pandas will create multi-level indices:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.pivot_table(more_grades, index=(\"name\", \"month\"), margins=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview functions\n",
    "When dealing with large `DataFrames`, it is useful to get a quick overview of its content. Pandas offers a few functions for this. First, let's create a large `DataFrame` with a mix of numeric values, missing values and text values. Notice how Jupyter displays only the corners of the `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "much_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26))\n",
    "large_df = pd.DataFrame(much_data, columns=list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "large_df[large_df % 16 == 0] = np.nan\n",
    "large_df.insert(3,\"some_text\", \"Blabla\")\n",
    "large_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `head()` method returns the top 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "large_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there's also a `tail()` function to view the bottom 5 rows. You can pass the number of rows you want:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "large_df.tail(n=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info()` method prints out a summary of each columns contents:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "large_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `describe()` method gives a nice overview of the main aggregated values over each column:\n",
    "* `count`: number of non-null (not NaN) values\n",
    "* `mean`: mean of non-null values\n",
    "* `std`: [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation) of non-null values\n",
    "* `min`: minimum of non-null values\n",
    "* `25%`, `50%`, `75%`: 25th, 50th and 75th [percentile](https://en.wikipedia.org/wiki/Percentile) of non-null values\n",
    "* `max`: maximum of non-null values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "large_df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving & loading\n",
    "Pandas can save `DataFrame`s to various backends, including file formats such as CSV, Excel, JSON, HTML and HDF5, or to a SQL database. Let's create a `DataFrame` to demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "my_df = pd.DataFrame(\n",
    "    [[\"Biking\", 68.5, 1985, np.nan], [\"Dancing\", 83.1, 1984, 3]], \n",
    "    columns=[\"hobby\",\"weight\",\"birthyear\",\"children\"],\n",
    "    index=[\"alice\", \"bob\"]\n",
    ")\n",
    "my_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving\n",
    "Let's save it to CSV, HTML and JSON:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "my_df.to_csv(\"my_df.csv\")\n",
    "my_df.to_html(\"my_df.html\")\n",
    "my_df.to_json(\"my_df.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Let's take a peek at what was saved:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"):\n",
    "    print(\"#\", filename)\n",
    "    with open(filename, \"rt\") as f:\n",
    "        print(f.read())\n",
    "        print()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the index is saved as the first column (with no name) in a CSV file, as `<th>` tags in HTML and as keys in JSON.\n",
    "\n",
    "Saving to other formats works very similarly, but some formats require extra libraries to be installed. For example, saving to Excel requires the openpyxl library:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    my_df.to_excel(\"my_df.xlsx\", sheet_name='People')\n",
    "except ImportError as e:\n",
    "    print(e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading\n",
    "Now let's load our CSV file back into a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "my_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0)\n",
    "my_df_loaded"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might guess, there are similar `read_json`, `read_html`, `read_excel` functions as well.  We can also read data straight from the Internet. For example, let's load the top 1,000 U.S. cities from github:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "us_cities = None\n",
    "try:\n",
    "    csv_url = \"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\"\n",
    "    us_cities = pd.read_csv(csv_url, index_col=0)\n",
    "    us_cities = us_cities.head()\n",
    "except IOError as e:\n",
    "    print(e)\n",
    "us_cities"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more options available, in particular regarding datetime format. Check out the [documentation](http://pandas.pydata.org/pandas-docs/stable/io.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining `DataFrame`s\n",
    "\n",
    "## SQL-like joins\n",
    "One powerful feature of pandas is it's ability to perform SQL-like joins on `DataFrame`s. Various types of joins are supported: inner joins, left/right outer joins and full joins. To illustrate this, let's start by creating a couple simple `DataFrame`s:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "city_loc = pd.DataFrame(\n",
    "    [\n",
    "        [\"CA\", \"San Francisco\", 37.781334, -122.416728],\n",
    "        [\"NY\", \"New York\", 40.705649, -74.008344],\n",
    "        [\"FL\", \"Miami\", 25.791100, -80.320733],\n",
    "        [\"OH\", \"Cleveland\", 41.473508, -81.739791],\n",
    "        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657]\n",
    "    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\n",
    "city_loc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "city_pop = pd.DataFrame(\n",
    "    [\n",
    "        [808976, \"San Francisco\", \"California\"],\n",
    "        [8363710, \"New York\", \"New-York\"],\n",
    "        [413201, \"Miami\", \"Florida\"],\n",
    "        [2242193, \"Houston\", \"Texas\"]\n",
    "    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"])\n",
    "city_pop"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's join these `DataFrame`s using the `merge()` function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.merge(left=city_loc, right=city_pop, on=\"city\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both `DataFrame`s have a column named `state`, so in the result they got renamed to `state_x` and `state_y`.\n",
    "\n",
    "Also, note that Cleveland, Salt Lake City and Houston were dropped because they don't exist in *both* `DataFrame`s. This is the equivalent of a SQL `INNER JOIN`. If you want a `FULL OUTER JOIN`, where no city gets dropped and `NaN` values are added, you must specify `how=\"outer\"`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\")\n",
    "all_cities"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course `LEFT OUTER JOIN` is also available by setting `how=\"left\"`: only the cities present in the left `DataFrame` end up in the result. Similarly, with `how=\"right\"` only cities in the right `DataFrame` appear in the result. For example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the key to join on is actually in one (or both) `DataFrame`'s index, you must use `left_index=True` and/or `right_index=True`. If the key column names differ, you must use `left_on` and `right_on`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "city_pop2 = city_pop.copy()\n",
    "city_pop2.columns = [\"population\", \"name\", \"state\"]\n",
    "pd.merge(left=city_loc, right=city_pop2, left_on=\"city\", right_on=\"name\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation\n",
    "Rather than joining `DataFrame`s, we may just want to concatenate them. That's what `concat()` is for:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "result_concat = pd.concat([city_loc, city_pop])\n",
    "result_concat"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this operation aligned the data horizontally (by columns) but not vertically (by rows). In this example, we end up with multiple rows having the same index (eg. 3). Pandas handles this rather gracefully:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "result_concat.loc[3]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can tell pandas to just ignore the index:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.concat([city_loc, city_pop], ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when a column does not exist in a `DataFrame`, it acts as if it was filled with `NaN` values. If we set `join=\"inner\"`, then only columns that exist in *both* `DataFrame`s are returned:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.concat([city_loc, city_pop], join=\"inner\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can concatenate `DataFrame`s horizontally instead of vertically by setting `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.concat([city_loc, city_pop], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it really does not make much sense because the indices do not align well (eg. Cleveland and San Francisco end up on the same row, because they shared the index label `3`). So let's reindex the `DataFrame`s by city name before concatenating:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.concat([city_loc.set_index(\"city\"), city_pop.set_index(\"city\")], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a lot like a `FULL OUTER JOIN`, except that the `state` columns were not renamed to `state_x` and `state_y`, and the `city` column is now the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `append()` method is a useful shorthand for concatenating `DataFrame`s vertically:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "city_loc.append(city_pop)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always in pandas, the `append()` method does *not* actually modify `city_loc`: it works on a copy and returns the modified copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories\n",
    "It is quite frequent to have values that represent categories, for example `1` for female and `2` for male, or `\"A\"` for Good, `\"B\"` for Average, `\"C\"` for Bad. These categorical values can be hard to read and cumbersome to handle, but fortunately pandas makes it easy. To illustrate this, let's take the `city_pop` `DataFrame` we created earlier, and add a column that represents a category:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "city_eco = city_pop.copy()\n",
    "city_eco[\"eco_code\"] = [17, 17, 34, 20]\n",
    "city_eco"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the `eco_code` column is full of apparently meaningless codes. Let's fix that. First, we will create a new categorical column based on the `eco_code`s:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "city_eco[\"economy\"] = city_eco[\"eco_code\"].astype('category')\n",
    "city_eco[\"economy\"].cat.categories"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can give each category a meaningful name:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "city_eco[\"economy\"].cat.categories = [\"Finance\", \"Energy\", \"Tourism\"]\n",
    "city_eco"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that categorical values are sorted according to their categorical order, *not* their alphabetical order:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "city_eco.sort_values(by=\"economy\", ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What next?\n",
    "As you probably noticed by now, pandas is quite a large library with *many* features. Although we went through the most important features, there is still a lot to discover. Probably the best way to learn more is to get your hands dirty with some real-life data. It is also a good idea to go through pandas' excellent [documentation](http://pandas.pydata.org/pandas-docs/stable/index.html), in particular the [Cookbook](http://pandas.pydata.org/pandas-docs/stable/cookbook.html)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_section_display": "none",
   "toc_threshold": 6,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

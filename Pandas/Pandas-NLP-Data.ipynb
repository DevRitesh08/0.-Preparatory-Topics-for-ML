{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NLP Project Steps",
   "id": "e6645d625d140e1a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from textblob.en import positive"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "We have already collected the data from Kaggle and stored it in DataSets folder"
   ],
   "id": "1f840d6af03e5f47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv('../DataSets/IMDB Dataset 50k review.csv')",
   "id": "fe25a1b636bebc94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "41bbeb57b588d13e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.info()\n",
    "# no missing values are present"
   ],
   "id": "44bf4fdb4eb939ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.duplicated().sum()\n",
    "# so we have 418 duplicate rows"
   ],
   "id": "1f154f86c765df9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.drop_duplicates(inplace=True)",
   "id": "86f381bffe07ab17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.duplicated().sum()",
   "id": "16d9f25c9cec744f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# df = df.iloc[:1000]     # taking only 1000 rows for faster processing .",
   "id": "40e1cbfa98eeb765",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "* We will perform the following steps to clean the data:\n",
    "    - Lower Case\n",
    "    - Remove trailing and leading spaces\n",
    "    - Remove HTML tags\n",
    "    - Remove URLs\n",
    "    - Expanding Abbreviations\n",
    "    - Spelling Correction\n",
    "    - Remove Punctuation"
   ],
   "id": "d1e6ef13d026bbac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lower Case\n",
    "df['review'] = df['review'].str.lower()\n",
    "df.head()"
   ],
   "id": "1bce97e4c6b34d46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove trailing and leading spaces\n",
    "df['review'] = df['review'].str.strip()"
   ],
   "id": "adf20ceae22c4afc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove HTML tags\n",
    "\n",
    "# method 1\n",
    "# import re                               # regular expression library\n",
    "# def remove_html_tags(data):\n",
    "#     data = re.sub(r'<.*?>','', data)    #\n",
    "#     return data\n",
    "\n",
    "# remove_html_tags('<p>This is a <b>bold</b> paragraph.</p>')\n",
    "\n",
    "# df['review'] = df['review'].apply(remove_html_tags)\n",
    "# df.head()\n",
    "\n",
    "# method 2 ( Better method )\n",
    "df['review'] = df['review'].str.replace(r'<.*?>', '', regex=True)   # here regex=True is important since we are using regex pattern without it it will give warning .\n",
    "df.head()"
   ],
   "id": "8126b4a4149b7368",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# finding reviews with one or more url in it\n",
    "df[df['review'].str.contains(r\"https?://\\S+|www\\.\\S+\")].iloc[1].values"
   ],
   "id": "b5008da0c9d1ef43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove URLs\n",
    "# http://google.in      www.google.com      www.india.gov.in\n",
    "# so we have multiple types of urls\n",
    "\n",
    "# Method 1\n",
    "import re\n",
    "\n",
    "def remove_url(data):\n",
    "    data = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", data)\n",
    "    return data\n",
    "\n",
    "# remove_url(\"Check out this link: https://example.com and also visit www.example.org for more info and http://test.com\")\n",
    "\n",
    "# df['review'] = df['review'].apply(remove_url)\n",
    "\n",
    "# Method 2 ( Better method )\n",
    "df['review'] = df['review'].str.replace(r\"https?://\\S+|www\\.\\S+\", \"\", regex=True)\n",
    "df.head()"
   ],
   "id": "5a7801dc4deab0db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# expanding abbreviations\n",
    "# he's  -> he is and so on\n",
    "# we will use regex for this task\n",
    "# we will create a function to do this task and then apply it to the dataframe\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_abb(data):\n",
    "    data = re.sub(r\"he's\", \"he is\", data)\n",
    "    data = re.sub(r\"there's\", \"there is\", data)\n",
    "    data = re.sub(r\"We're\", \"We are\", data)\n",
    "    data = re.sub(r\"That’s\", \"That is\", data)\n",
    "    data = re.sub(r\"won’t\", \"will not\", data)\n",
    "    data = re.sub(r\"they’re\", \"they are\", data)\n",
    "    data = re.sub(r\"Can’t\", \"Cannot\", data)\n",
    "    data = re.sub(r\"wasn’t\", \"was not\", data)\n",
    "    data = re.sub(r\"don\\x89Ûat\", \"do not\", data)\n",
    "    data = re.sub(r\"aren’t\", \"are not\", data)\n",
    "    data = re.sub(r\"isn’t\", \"is not\", data)\n",
    "    data = re.sub(r\"What's\", \"What is\", data)\n",
    "    data = re.sub(r\"haven’t\", \"have not\", data)\n",
    "    data = re.sub(r\"hasn’t\", \"has not\", data)\n",
    "    data = re.sub(r\"There’s\", \"There is\", data)\n",
    "    data = re.sub(r\"He’s\", \"He is\", data)\n",
    "    data = re.sub(r\"It’s\", \"It is\", data)\n",
    "    data = re.sub(r\"You’re\", \"You are\", data)\n",
    "    data = re.sub(r\"I’M\", \"I am\", data)\n",
    "    data = re.sub(r\"shouldn’t\", \"should not\", data)\n",
    "    data = re.sub(r\"wouldn’t\", \"would not\", data)\n",
    "    data = re.sub(r\"i’m\", \"I am\", data)\n",
    "    data = re.sub(r\"I\\x89ÛÏm\", \"I am\", data)\n",
    "    data = re.sub(r\"I\\x89Û÷m\", \"I am\", data)\n",
    "    data = re.sub(r\"Isn’t\", \"is not\", data)\n",
    "    data = re.sub(r\"Here’s\", \"Here is\", data)\n",
    "    data = re.sub(r\"You’ve\", \"you have\", data)\n",
    "    data = re.sub(r\"you\\x89Ûªve\", \"you have\", data)\n",
    "    data = re.sub(r\"We’re\", \"we are\", data)\n",
    "    data = re.sub(r\"what’s\", \"what is\", data)\n",
    "    data = re.sub(r\"couldn’t\", \"could not\", data)\n",
    "    data = re.sub(r\"we’ve\", \"we have\", data)\n",
    "    data = re.sub(r\"it\\x89Ûªs\", \"it is\", data)\n",
    "    data = re.sub(r\"doesn\\x89Ûªt\", \"does not\", data)\n",
    "    data = re.sub(r\"It\\x89Ûªs\", \"It is\", data)\n",
    "    data = re.sub(r\"Here\\x89Ûªs\", \"Here is\", data)\n",
    "    data = re.sub(r\"who’s\", \"who is\", data)\n",
    "    data = re.sub(r\"I\\x89Ûªve\", \"I have\", data)\n",
    "    data = re.sub(r\"y’all\", \"you all\", data)\n",
    "    data = re.sub(r\"can\\x89Ûªt\", \"cannot\", data)\n",
    "    data = re.sub(r\"would\\x89Ûªve\", \"would have\", data)\n",
    "    data = re.sub(r\"it’ll\", \"it will\", data)\n",
    "    data = re.sub(r\"we’ll\", \"we will\", data)\n",
    "    data = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", data)\n",
    "    data = re.sub(r\"We\\x89Ûªve\", \"We have\", data)\n",
    "    data = re.sub(r\"he’ll\", \"he will\", data)\n",
    "    data = re.sub(r\"Y’all\", \"You all\", data)\n",
    "    data = re.sub(r\"Weren’t\", \"Were not\", data)\n",
    "    data = re.sub(r\"Didn’t\", \"Did not\", data)\n",
    "    data = re.sub(r\"they’ll\", \"they will\", data)\n",
    "    data = re.sub(r\"they’d\", \"they would\", data)\n",
    "    data = re.sub(r\"DON’T\", \"DO NOT\", data)\n",
    "    data = re.sub(r\"That\\x89Ûªs\", \"That is\", data)\n",
    "    data = re.sub(r\"they’ve\", \"they have\", data)\n",
    "    data = re.sub(r\"i’d\", \"I would\", data)\n",
    "    data = re.sub(r\"should\\x89Ûªve\", \"should have\", data)\n",
    "    data = re.sub(r\"You\\x89Ûªre\", \"You are\", data)\n",
    "    data = re.sub(r\"where’s\", \"where is\", data)\n",
    "    data = re.sub(r\"Don\\x89Ûªt\", \"Do not\", data)\n",
    "    data = re.sub(r\"We’d\", \"We would\", data)\n",
    "    data = re.sub(r\"i’ll\", \"I will\", data)\n",
    "    data = re.sub(r\"weren’t\", \"were not\", data)\n",
    "    data = re.sub(r\"They’re\", \"They are\", data)\n",
    "    data = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", data)\n",
    "    data = re.sub(r\"you\\x89Ûªll\", \"you will\", data)\n",
    "    data = re.sub(r\"I\\x89Ûªd\", \"I would\", data)\n",
    "    data = re.sub(r\"let’s\", \"let us\", data)\n",
    "    data = re.sub(r\"it’s\", \"it is\", data)\n",
    "    data = re.sub(r\"can’t\", \"cannot\", data)\n",
    "    data = re.sub(r\"don’t\", \"do not\", data)\n",
    "    data = re.sub(r\"you’re\", \"you are\", data)\n",
    "    data = re.sub(r\"i’ve\", \"I have\", data)\n",
    "    data = re.sub(r\"that’s\", \"that is\", data)\n",
    "    data = re.sub(r\"i’ll\", \"I will\", data)\n",
    "    data = re.sub(r\"doesn’t\", \"does not\", data)\n",
    "    data = re.sub(r\"i’d\", \"I would\", data)\n",
    "    data = re.sub(r\"don't\", \"do not\", data)\n",
    "    data = re.sub(r\"you're\", \"you are\", data)\n",
    "    data = re.sub(r\"i've\", \"I have\", data)\n",
    "    data = re.sub(r\"that's\", \"that is\", data)\n",
    "    data = re.sub(r\"i'll\", \"I will\", data)\n",
    "    data = re.sub(r\"doesn't\", \"does not\", data)\n",
    "    data = re.sub(r\"i'd\", \"I would\", data)\n",
    "    data = re.sub(r\"didn't\", \"did not\", data)\n",
    "    data = re.sub(r\"ain't\", \"am not\", data)\n",
    "    data = re.sub(r\"you'll\", \"you will\", data)\n",
    "    data = re.sub(r\"I've\", \"I have\", data)\n",
    "    data = re.sub(r\"Don't\", \"do not\", data)\n",
    "    data = re.sub(r\"I'll\", \"I will\", data)\n",
    "    data = re.sub(r\"I'd\", \"I would\", data)\n",
    "    data = re.sub(r\"Let's\", \"Let us\", data)\n",
    "    data = re.sub(r\"you'd\", \"You would\", data)\n",
    "    data = re.sub(r\"It's\", \"It is\", data)\n",
    "    data = re.sub(r\"Ain't\", \"am not\", data)\n",
    "    data = re.sub(r\"Haven't\", \"Have not\", data)\n",
    "    data = re.sub(r\"Could've\", \"Could have\", data)\n",
    "    data = re.sub(r\"you've\", \"you have\", data)\n",
    "    data = re.sub(r\"donâ't\", \"do not\", data)\n",
    "\n",
    "    return data"
   ],
   "id": "8fdbb0091d36cd41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "remove_abb(\"He's going to the park. There's a dog. I'm happy. Don't worry!\")",
   "id": "32f914c81ad8dc1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['review'] = df['review'].apply(remove_abb)\n",
    "df.head()"
   ],
   "id": "eed557518ca51369",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Spelling Correction\n",
    "# we will use textblob library for this task\n",
    "# !pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# text = \"hi i can drve at nigt with no ligts\"\n",
    "# TextBlob(text).correct().string\n",
    "\n",
    "def correct_spell(data):\n",
    "    return TextBlob(data).correct().string"
   ],
   "id": "9331c2763c1971fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# df['review'] = df['review'].apply(correct_spell)    # this will take some time to execute",
   "id": "156f95c20ac6349d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove Punctuation\n",
    "import string\n",
    "string.punctuation"
   ],
   "id": "64db757dfbba953a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_punctuation(data):\n",
    "    for char in string.punctuation:\n",
    "        if char in data:\n",
    "            data = data.replace(char, '')\n",
    "    return data"
   ],
   "id": "e939907725a0b2f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# remove_punctuation(\"Hello, world! is this a test?\")",
   "id": "a574050b59ead192",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['review'] = df['review'].apply(remove_punctuation)\n",
    "df.head()"
   ],
   "id": "b4abc922129a208b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "- we will perform the following steps to preprocess the text:\n",
    "    - Tokenization : it is the process of breaking down a text into smaller units called tokens . tokens can be words , phrases , or sentences .\n",
    "    - Stop Words Removal : these are the most common words in a language that do not add much meaning to a sentence for example : is, am, are, the, a, an, in, on, at, for, to, and, but, or etc .\n",
    "    - Stemming : it reduces words to their root form , but the root form may not be a valid word .\n",
    "    - Lemmatization : it reduces words to their base form , and the base form is a valid word ."
   ],
   "id": "b4098ebc6e25ac10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "cb351f5d54cf3ede",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "df['Tokenized_review'] = df['review'].apply(word_tokenize)\n",
    "df.head()"
   ],
   "id": "94dd473b96eb53d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stop Words Removal\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ],
   "id": "63194c47368dbcd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(stopwords.words('english'))",
   "id": "8078d07640f9d077",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_stopwords(text):\n",
    "    L =[]\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            L.append(word)\n",
    "    return L\n",
    "\n",
    "# The actual time complexity is O(n * m * k), where: n = number of reviews , m = average number of words per review , k = number of stop words (about 179)\n",
    "# But if you convert stop_words to a set, checking membership becomes O(1), so the complexity is O(n * m).\n",
    "\n",
    "# stop_words_set = set(stop_words)\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     return [word for word in text if word not in stop_words_set]\n"
   ],
   "id": "85d697ff67849d82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# remove_stopwords(['i', 'thought', 'this', 'was', 'a' , 'wonderful', 'way'])",
   "id": "e83d47d50fa3efaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['Tokenized_review'] = df['Tokenized_review'].apply(remove_stopwords)      # removing stop words from the tokenized review .\n",
    "df.head()"
   ],
   "id": "c5d0649fe02fefa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.  EDA ( Exploratory Data Analysis ) and Feature Engineering\n",
    "\n",
    "- we will perform the following steps to analyze the data:\n",
    "    - distribution of text length / word count\n",
    "    - common unigrams/bigrams/trigrams\n",
    "    - wordcloud"
   ],
   "id": "a099e19eb72bab0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution of text length / word count\n",
    "# we will create two new columns in the dataframe one for character length and other for word length to analyze the distribution of text length / word count ."
   ],
   "id": "6d9fbba134956011",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['review'] = df['Tokenized_review'].apply(lambda x : \" \".join(x))\n",
    "df.head()"
   ],
   "id": "d011e7d9230f3565",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['char_len'] = df['review'].str.len()",
   "id": "3562931e08203303",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "116cf90fd487bebb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['word_len'] = df['Tokenized_review'].apply(len)",
   "id": "70ca59a15e471558",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "44c486faa43fb8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.displot(df['char_len'])"
   ],
   "id": "b2efe4680055dc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[ df['sentiment'] == 'positive' ]",
   "id": "7799641a53b4a046",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.displot(df[ df['sentiment'] == 'positive' ]['char_len'])\n",
    "sns.displot(df[ df['sentiment'] == 'negative' ]['char_len'])"
   ],
   "id": "8142ce903ef62902",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sns.displot creates a new figure for each call, so each sentiment is plotted separately.\n",
    "# To show both distributions on a single plot, use sns.histplot and plot both series together, specifying the hue parameter.\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(data=df, x='char_len', hue='sentiment', kde=True)"
   ],
   "id": "1bee1e91ebef5866",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "if we compare both the plots and see the difference then we can say that the positive reviews are generally longer than the negative reviews  but here we are not able to see the difference clearly . so , it is not a good feature to classify the reviews .",
   "id": "927c88b00065f6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sns.histplot(data=df, x='word_len', hue='sentiment', kde=True)",
   "id": "d61f2089ce4ef7ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "this too is not a good feature to classify the reviews .",
   "id": "4a13dd14bc11678a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Common unigrams/bigrams/trigrams and so on .\n",
    "    # unigrams : single words in the text\n",
    "    # bigrams : two consecutive words in the text\n",
    "    # trigrams : three consecutive words in the text"
   ],
   "id": "e576078c8c51a8cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "84209de4d2f2006",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This will take too much time to execute since we have 50k reviews in the dataset . So , we will take only 1000 reviews for faster processing .",
   "id": "1005e9020f150b0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp = df.copy()\n",
    "df = df.iloc[:10000]\n",
    "# this will prevent us from running the whole notebook again and again . we have stored the original dataframe in temp variable and it can be used whenever needed ."
   ],
   "id": "5f39e10cb52ebd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk import ngrams\n",
    "# df['Tokenized_review'].sum()    # this will give a list of all the words in the reviews .\n",
    "pd.Series(ngrams(df['Tokenized_review'].sum(), 3)).value_counts().head(10)"
   ],
   "id": "e5e645b2b35e97cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# analyzing positive reviews in trigrams\n",
    "positive_3grams = pd.Series(ngrams(df[df['sentiment'] == 'positive']['Tokenized_review'].sum(), 3)).value_counts().head(10)\n",
    "# analyzing negative reviews in trigrams\n",
    "negative_3grams = pd.Series(ngrams(df[df['sentiment'] == 'negative']['Tokenized_review'].sum(), 3)).value_counts().head(10)\n",
    "# now we can see the difference between the positive and negative reviews .\n",
    "sns.histplot(positive_3grams)\n",
    "sns.histplot(negative_3grams)\n",
    "# showing both the plots in a single plot\n",
    "sns.histplot(data=positive_3grams, color='blue', label='Positive Reviews', kde=True)\n",
    "sns.histplot(data=negative_3grams, color='red', label='Negative Reviews', kde=True)\n",
    "plt.legend()"
   ],
   "id": "6d1adbd6342b65c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Wordcloud\n",
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (20,20)) # Positive Review Text\n",
    "wc = WordCloud(width = 1600, height = 800).generate(\" \".join(df[df['sentiment'] == 'positive']['review']))\n",
    "plt.imshow(wc)"
   ],
   "id": "f2f123790ec5fe99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize = (20,20)) # Positive Review Text\n",
    "wc = WordCloud(width = 1600, height = 800).generate(\" \".join(df[df['sentiment'] == 'negative']['review']))\n",
    "plt.imshow(wc)"
   ],
   "id": "2cca758c5793abce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Vectorization\n",
    "- we will perform the following steps to vectorize the text:\n",
    "    - bag of words : it is a simple and commonly used method for text vectorization . it represents a text as a bag of its words , disregarding grammar and word order but keeping multiplicity ."
   ],
   "id": "b0e35b51b369cb57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "c343724d0b7e86d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install scikit-learn",
   "id": "4e2bbbb74db263c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=5000 , ngram_range=(3,3))   # we are using trigrams here , nrange=(1,1) for unigrams , (2,2) for bigrams and so on . and max_features is used to limit the number of features to 5000 most frequent words .\n",
    "# ngram_range=(1,3) will consider unigrams , bigrams and trigrams all together , but it will increase the number of features too much .\n",
    "bag_of_words = count_vectorizer.fit_transform(df['review'])\n",
    "bag_of_words = pd.DataFrame(bag_of_words.toarray(), columns=count_vectorizer.get_feature_names_out())"
   ],
   "id": "2e962477fa0ecba5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bag_of_words",
   "id": "a7a9c72820f2be32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(bag_of_words.values)"
   ],
   "id": "dea29628bf0c884a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pca_result.shape",
   "id": "ec522ab2b56eb0ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1], hue=df['sentiment'])",
   "id": "aee859b6cf9aea04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "110e1520d993370",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
